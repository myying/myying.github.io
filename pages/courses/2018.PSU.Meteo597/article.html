<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<meta name="author" content="Yue Ying">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="../../../style.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-148955805-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-148955805-2');
</script>

<!-- MathJax -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<title>PSU Meteo587 DA Course</title>
</head>
<body>

<div class="container">
<div class="page" id="about">
<div class="content">
<h1>Data assimilation (Meteo 597, Penn State Dept. Meteorology and Atmospheric Science, Spring 2018)</h1>
      <p>
      Data assimilation seeks to find the best estimate of the state of a dynamical system and its uncertainty by combining information from model forecasts and observations. It has been adopted for state and parameter estimation for a wide range of dynamical systems across many disciplines such as weather, ocean, land, water, air quality, climate, ecosystem and astrophysics. This course covers various data assimilation approaches such as variational, ensemble-based, hybrid and nonlinear methods. </p>
      <p>Co-instructors: Steve Greybush, Fuqing Zhang</p>
      <p>More information can be found in the <a href="syllabus.pdf">Course Syllabus</a>.</p>
      <p>
      Lecture 1: Overview of data assimilation.<br/>
      Lecture 2: Least squares approach.
      <a href="Lecture02_Least_Squares.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 3: Observation operators.
      <a href="Lecture03_H_Operator.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 4: Optimal interpolation.
      <a href="Lecture04_OI.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 5: Bayesian approach.
      <a href="Lecture05_Bayesian_Approach.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 6: 3DVar method.
      <a href="Lecture06_3DVar.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 7: Minimization algorithms.
      <a href="Lecture07_Minimization.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 8: Preconditioning.
      <a href="Lecture08_Preconditioning.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 9: Nonlinear test models.
      <a href="Lecture09_Lorenz_Models.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 10: Tangent linear model.
      <a href="Lecture10_Tangent_Linear_Model.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 11: Adjoint model.
      <a href="Lecture11_Adjoint_Model.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 12: Characterizing dynamical error growth.
      <a href="Lecture12_Characterize_Error_Growth.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 13: Kalman filter.
      <a href="Lecture13_Kalman_Filter.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 14: Extended Kalman filter.
      <a href="Lecture14_Extended_Kalman_Filter.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 15: Ensemble Kalman filter (EnKF).
      <a href="Lecture15_EnKF.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 16: A serial variant of EnKF.
      <a href="Lecture16_Serial_EnKF.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 17: Square root filters.
      <a href="Lecture17_Square_Root_Modification.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 18: Practical Modifications to EnKF: inflation and localization. <br/>
      Lecture 19: Innovation statistics.
      <a href="Lecture19_Innovation_Statistics.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 20: Adaptive localization and inflation.
      <a href="Lecture20_Adaptive_Localization_Inflation.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 21: Ensemble transform Kalman filter (ETKF).
      <a href="Lecture21_ETKF.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 22: LETKF. <br/>
      Lecture 23: Parameter estimation.
      <a href="Lecture23_Parameter_Estimation.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 24: 4DVar method.
      <a href="Lecture24_4DVar.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lectures 25,26: Hybrid methods.
      <a href="Lecture25_Hybrid_Methods.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 27: Assimilation system design. <br/>
      Lecture 28: Adjoint and ensemble sensitivity.
      <a href="Lecture28_Adjoint_Ensemble_Sensitivity.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 29: Observation impact.
      <a href="Lecture29_Observation_Impact.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 30: Operational data assimilation systems. <br/>
      Lecture 31: Radar data assimilation. <br/>
      Lecture 32: Regional-scale ensemble data assimilation. <br/>
      Lecture 33: Representation errors.
      <a href="Lecture33_Representation_Errors.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lecture 34: Particle filters.
      <a href="Lecture34_Particle_Filter.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      Lectures 35,36: Non-Gaussian methods.
      <a href="Lecture35,36_Non-Gaussian_Filters.pdf"><font size=2><i class="fa fa-sticky-note"></i>Notes</a></font><br/>
      <p> Homeworks: &nbsp;
      <a href="HW1.pdf"><i class="fa fa-edit"></i>1</a>, &nbsp;
      <a href="HW2.pdf"><i class="fa fa-edit"></i>2</a>, &nbsp;
      <a href="HW3.pdf"><i class="fa fa-edit"></i>3</a> </p>

</div>
</div></div>
<div class="footer">&copy; Yue Ying</div>
</body>
</html>
